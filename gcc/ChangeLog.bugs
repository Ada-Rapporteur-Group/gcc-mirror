==================== Branch work191-bugs, patch #210 ====================

Fix PR 118541, do not generate unordered fp cmoves.

2025-01-23  Michael Meissner  <meissner@linux.ibm.com>

gcc/

	PR target/118541
	* config/rs6000/rs6000.cc (rs6000_reverse_condition): Do not allow
	unordered floating point comparisons to be reversed if signaling NaNs
	are a possibility.
	(rs6000_maybe_emit_fp_cmove): Likewise.
	(rs6000_emit_cmove): Likewise.

==================== Branch work191-bugs, patch #202 ====================

PR target/108958 -- use mtvsrdd to zero extend GPR DImode to VSX TImode

Previously GCC would zero externd a DImode GPR value to TImode by first zero
extending the DImode value into a GPR TImode value, and then do a MTVSRDD to
move this value to a VSX register.

This patch does the move directly, since if the middle argument to MTVSRDD is 0,
it does the zero extend.

If the DImode value is already in a vector register, it does a XXSPLTIB and
XXPERMDI to get the value into the bottom 64-bits of the register.

I have built GCC with the patches in this patch set applied on both little and
big endian PowerPC systems and there were no regressions.  Can I apply this
patch to GCC 15?

2025-01-16  Michael Meissner  <meissner@linux.ibm.com>

gcc/

	PR target/108598
	* gcc/config/rs6000/rs6000.md (zero_extendditi2): New insn.

gcc/testsuite/

	PR target/108598
	* gcc.target/powerpc/pr108958.c: New test.

==================== Branch work191-bugs, patch #201 ====================

Add power9 and power10 float to logical optimizations.

I was answering an email from a co-worker and I pointed him to work I had done
for the Power8 era that optimizes the 32-bit float math library in Glibc.  In
doing so, I discovered with the Power9 and later computers, this optimization
is no longer taking place.

The glibc 32-bit floating point math functions have code that looks like:

	union u {
	  float f;
	  uint32_t u32;
	};

	float
	math_foo (float x, unsigned int mask)
	{
	  union u arg;
	  float x2;

	  arg.f = x;
	  arg.u32 &= mask;

	  x2 = arg.f;
	  /* ... */
	}

On power8 with the optimization it generates:

        xscvdpspn 0,1
        sldi 9,4,32
        mtvsrd 32,9
        xxland 1,0,32
        xscvspdpn 1,1

I.e., it converts the SFmode to the memory format (instead of the DFmode that
is used within the register), converts the mask so that it is in the vector
register in the upper 32-bits, and does a XXLAND (i.e. there is only one direct
move from GPR to vector register).  Then after doing this, it converts the
upper 32-bits back to DFmode.

If the XSCVSPDN instruction took the value in the normal 32-bit scalar in a
vector register, we wouldn't have needed the SLDI of the mask.

On power9/power10/power11 it currently generates:

        xscvdpspn 0,1
        mfvsrwz 2,0
        and 2,2,4
        mtvsrws 1,2
        xscvspdpn 1,1
        blr

I.e convert to SFmode representation, move the value to a GPR, do an AND
operation, move the 32-bit value with a splat, and then convert it back to
DFmode format.

With this patch, it now generates:

        xscvdpspn 0,1
        mtvsrwz 32,2
        xxland 32,0,32
        xxspltw 1,32,1
        xscvspdpn 1,1
        blr

I.e. convert to SFmode representation, move the mask to the vector register, do
the operation using XXLAND.  Splat the value to get the value in the correct
location, and then convert back to DFmode.

I have built GCC with the patches in this patch set applied on both little and
big endian PowerPC systems and there were no regressions.  Can I apply this
patch to GCC 15?

2025-01-16  Michael Meissner  <meissner@linux.ibm.com>

gcc/

	PR target/117487
	* config/rs6000/vsx.md (SFmode logical peephoole): Update comments in
	the original code that supports power8.  Add a new define_peephole2 to
	do the optimization on power9/power10.

==================== Branch work191-bugs, patch #200 ====================

PR 99293: Optimize splat of a V2DF/V2DI extract with constant element

We had optimizations for splat of a vector extract for the other vector
types, but we missed having one for V2DI and V2DF.  This patch adds a
combiner insn to do this optimization.

In looking at the source, we had similar optimizations for V4SI and V4SF
extract and splats, but we missed doing V2DI/V2DF.

Without the patch for the code:

	vector long long splat_dup_l_0 (vector long long v)
	{
	  return __builtin_vec_splats (__builtin_vec_extract (v, 0));
	}

the compiler generates (on a little endian power9):

	splat_dup_l_0:
		mfvsrld 9,34
	        mtvsrdd 34,9,9
		blr

Now it generates:

	splat_dup_l_0:
		xxpermdi 34,34,34,3
	        blr

2025-01-16  Michael Meissner  <meissner@linux.ibm.com>

gcc/

	PR target/99293
	* config/rs6000/vsx.md (vsx_splat_extract_<mode>): New insn.

gcc/testsuite/

	PR target/99293
	* gcc.target/powerpc/builtins-1.c: Adjust insn count.
	* gcc.target/powerpc/pr99293.c: New test.

==================== Branch work191-bugs, baseline ====================

Add ChangeLog.bugs and update REVISION.

2025-01-16  Michael Meissner  <meissner@linux.ibm.com>

gcc/

	* ChangeLog.bugs: New file for branch.
	* REVISION: Update.

2025-01-16   Michael Meissner  <meissner@linux.ibm.com>

	Clone branch
